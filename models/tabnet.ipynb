{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d912338",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, precision_score,\n",
    "    recall_score, f1_score, accuracy_score\n",
    ")\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de1f854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# 1) Load & prepare training data\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "train_path = \"./data/tabnet_train.csv\"\n",
    "test_path  = \"./data/tabnet_test.csv\"\n",
    "\n",
    "df_train = pd.read_csv(train_path, index_col=0)\n",
    "target   = \"PD-L1\"\n",
    "df_train[target] = df_train[target].astype(int)\n",
    "\n",
    "# Columns to embed vs scale\n",
    "cat_cols        = ['A','B','C','D','E','F']\n",
    "numeric_cols    = [c for c in df_train.columns if c not in cat_cols + [target]]\n",
    "\n",
    "# 1a) Fit LabelEncoders on categorical columns\n",
    "cat_encoders = {}\n",
    "for col in cat_cols:\n",
    "    le = LabelEncoder().fit(df_train[col].astype(str))\n",
    "    df_train[col] = le.transform(df_train[col].astype(str))\n",
    "    cat_encoders[col] = le\n",
    "\n",
    "# 1b) Fit StandardScaler on numeric columns\n",
    "scaler = StandardScaler().fit(df_train[numeric_cols])\n",
    "df_train[numeric_cols] = scaler.transform(df_train[numeric_cols])\n",
    "\n",
    "# Split into feature matrix X and target y\n",
    "X = df_train.drop(columns=[target])\n",
    "y = df_train[target].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fa44a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# 2) Compute TabNet embedding info\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "cat_idxs    = [X.columns.get_loc(c) for c in cat_cols]\n",
    "cat_dims    = [df_train[c].nunique() for c in cat_cols]\n",
    "cat_emb_dim = [min(50, (dim+1)//2) for dim in cat_dims]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22016ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# 3) 5‑Fold CV with weighted metrics\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "metrics_cv = {m: [] for m in [\"auc\",\"precision\",\"recall\",\"f1\",\"accuracy\"]}\n",
    "\n",
    "for fold, (tr_idx, va_idx) in enumerate(skf.split(X.values, y), start=1):\n",
    "    X_tr, X_va = X.values[tr_idx], X.values[va_idx]\n",
    "    y_tr, y_va = y[tr_idx], y[va_idx]\n",
    "\n",
    "    clf = TabNetClassifier(\n",
    "        cat_idxs         = cat_idxs,\n",
    "        cat_dims         = cat_dims,\n",
    "        cat_emb_dim      = cat_emb_dim,\n",
    "        optimizer_fn     = torch.optim.Adam,\n",
    "        optimizer_params = dict(lr=2e-2),\n",
    "        scheduler_fn     = torch.optim.lr_scheduler.StepLR,\n",
    "        scheduler_params = {\"step_size\":10, \"gamma\":0.9},\n",
    "        device_name      = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    )\n",
    "\n",
    "    clf.fit(\n",
    "        X_tr, y_tr,\n",
    "        eval_set       = [(X_va, y_va)],\n",
    "        eval_name      = [\"val\"],\n",
    "        eval_metric    = [\"accuracy\"],   # multiclass early-stop\n",
    "        max_epochs     = 100,\n",
    "        patience       = 20,\n",
    "        batch_size     = 2048,\n",
    "        virtual_batch_size = 512,\n",
    "        num_workers    = 4,\n",
    "        drop_last      = False\n",
    "    )\n",
    "\n",
    "    y_proba = clf.predict_proba(X_va)\n",
    "    y_pred  = clf.predict(X_va)\n",
    "\n",
    "    fold_metrics = {\n",
    "        \"auc\":       roc_auc_score(y_va, y_proba, multi_class=\"ovr\", average=\"weighted\"),\n",
    "        \"precision\": precision_score(y_va, y_pred, average=\"weighted\"),\n",
    "        \"recall\":    recall_score(y_va, y_pred, average=\"weighted\"),\n",
    "        \"f1\":        f1_score(y_va, y_pred, average=\"weighted\"),\n",
    "        \"accuracy\":  accuracy_score(y_va, y_pred)\n",
    "    }\n",
    "\n",
    "    print(f\"Fold {fold} → \" +\n",
    "          \", \".join(f\"{k}={v:.4f}\" for k,v in fold_metrics.items()))\n",
    "\n",
    "    for k,v in fold_metrics.items():\n",
    "        metrics_cv[k].append(v)\n",
    "\n",
    "print(\"\\n5‑Fold CV Summary:\")\n",
    "for k, vals in metrics_cv.items():\n",
    "    print(f\"{k:>9}: mean={np.mean(vals):.4f}, min={np.min(vals):.4f}, max={np.max(vals):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f118ff68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# 4) Train final model on all training data\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "final_clf = TabNetClassifier(\n",
    "    cat_idxs         = cat_idxs,\n",
    "    cat_dims         = cat_dims,\n",
    "    cat_emb_dim      = cat_emb_dim,\n",
    "    optimizer_fn     = torch.optim.Adam,\n",
    "    optimizer_params = dict(lr=2e-2),\n",
    "    scheduler_fn     = torch.optim.lr_scheduler.StepLR,\n",
    "    scheduler_params = {\"step_size\":10, \"gamma\":0.9},\n",
    "    device_name      = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")\n",
    "final_clf.fit(\n",
    "    X.values, y,\n",
    "    max_epochs         = 100,\n",
    "    patience           = 20,\n",
    "    batch_size         = 2048,\n",
    "    virtual_batch_size = 512,\n",
    "    num_workers        = 4,\n",
    "    drop_last          = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e47a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# 5) Bootstrap evaluation on hold‑out set (95% CI)\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "df_hold = pd.read_csv(test_path, index_col=0)\n",
    "df_hold[target] = df_hold[target].astype(int)\n",
    "\n",
    "# 5a) Encode categorical columns\n",
    "for col, le in cat_encoders.items():\n",
    "    unseen = set(df_hold[col].astype(str)) - set(le.classes_)\n",
    "    if unseen:\n",
    "        raise ValueError(f\"Hold‑out has unseen labels in '{col}': {unseen}\")\n",
    "    df_hold[col] = le.transform(df_hold[col].astype(str))\n",
    "\n",
    "# 5b) Scale numeric columns\n",
    "df_hold[numeric_cols] = scaler.transform(df_hold[numeric_cols])\n",
    "\n",
    "X_hold = df_hold.drop(columns=[target]).values\n",
    "y_hold = df_hold[target].values\n",
    "\n",
    "# 5c) Single-shot predictions\n",
    "y_proba = final_clf.predict_proba(X_hold)\n",
    "y_pred  = final_clf.predict(X_hold)\n",
    "\n",
    "# 5d) Bootstrap metrics\n",
    "n_boot = 1000\n",
    "n      = len(y_hold)\n",
    "metrics_bo = {m: [] for m in metrics_cv}\n",
    "\n",
    "for _ in range(n_boot):\n",
    "    idx = np.random.randint(0, n, n)\n",
    "    yt, yp, yv = y_hold[idx], y_pred[idx], y_proba[idx]\n",
    "    metrics_bo[\"accuracy\"].append( accuracy_score(yt, yp) )\n",
    "    metrics_bo[\"precision\"].append(\n",
    "        precision_score(yt, yp, average=\"weighted\") )\n",
    "    metrics_bo[\"recall\"].append(\n",
    "        recall_score(yt, yp, average=\"weighted\") )\n",
    "    metrics_bo[\"f1\"].append(\n",
    "        f1_score(yt, yp, average=\"weighted\") )\n",
    "    metrics_bo[\"auc\"].append(\n",
    "        roc_auc_score(yt, yv, multi_class=\"ovr\", average=\"weighted\") )\n",
    "\n",
    "print(\"\\nHold‑out bootstrap (1 000 samples):\")\n",
    "for k, vals in metrics_bo.items():\n",
    "    mean = np.mean(vals)\n",
    "    lo, hi = np.percentile(vals, [2.5, 97.5])\n",
    "    print(f\"{k:>9}: {mean:.4f} (95% CI {lo:.4f}–{hi:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ad4783",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import (\n",
    "    roc_curve,\n",
    "    auc,\n",
    "    roc_auc_score,\n",
    "    confusion_matrix,\n",
    "    precision_recall_fscore_support\n",
    ")\n",
    "\n",
    "def calculate_sensitivity_specificity(\n",
    "    df,\n",
    "    true_label_col='true_label',\n",
    "    pred_label_col='prediction_label',\n",
    "    class_names=None\n",
    "):\n",
    "    # Get sorted list of classes\n",
    "    classes = sorted(df[true_label_col].unique())\n",
    "    # Default names\n",
    "    if class_names is None:\n",
    "        class_names = [f'Class {cls}' for cls in classes]\n",
    "\n",
    "    y_true = df[true_label_col].values\n",
    "    y_pred = df[pred_label_col].values\n",
    "\n",
    "    sens_spec = {}\n",
    "    for cls_val, cls_name in zip(classes, class_names):\n",
    "        y_true_bin = (y_true == cls_val).astype(int)\n",
    "        y_pred_bin = (y_pred == cls_val).astype(int)\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true_bin, y_pred_bin).ravel()\n",
    "        sens = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "        spec = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
    "        sens_spec[cls_name] = {\n",
    "            'sensitivity': sens,\n",
    "            'specificity': spec,\n",
    "            'tp': tp, 'tn': tn, 'fp': fp, 'fn': fn,\n",
    "            'class_value': cls_val\n",
    "        }\n",
    "    return sens_spec\n",
    "\n",
    "def plot_multiclass_roc_with_metrics(\n",
    "    df,\n",
    "    true_label_col='true_label',\n",
    "    pred_label_col='prediction_label',\n",
    "    # Now expects a list of one column name per class\n",
    "    pred_score_cols=None,\n",
    "    class_names=None,\n",
    "    colors=None,\n",
    "    font_family='Arial Black',\n",
    "    font_size_title=16,\n",
    "    font_size_axes=12,\n",
    "    font_size_ticks=7,\n",
    "    save_path='multiclass_roc_curves.svg',\n",
    "    show_legend=False,\n",
    "    print_detailed_metrics=True\n",
    "):\n",
    "    # 1) Prepare classes and binarized true labels\n",
    "    classes = sorted(df[true_label_col].unique())\n",
    "    n_classes = len(classes)\n",
    "    if class_names is None:\n",
    "        class_names = [f'Class {cls}' for cls in classes]\n",
    "\n",
    "    y_true = df[true_label_col].values\n",
    "    y_true_bin = label_binarize(y_true, classes=classes)\n",
    "    if y_true_bin.ndim == 1:\n",
    "        y_true_bin = np.column_stack([1 - y_true_bin, y_true_bin])\n",
    "\n",
    "    # 2) Determine which probability columns to use\n",
    "    if pred_score_cols is None:\n",
    "        # infer any column starting with 'Score_'\n",
    "        pred_score_cols = [c for c in df.columns if c.startswith('Score_')]\n",
    "    assert len(pred_score_cols) == n_classes, \\\n",
    "        f\"Need {n_classes} score columns, got {len(pred_score_cols)}\"\n",
    "\n",
    "    # 3) Sensitivity & Specificity printout\n",
    "    print(\"=\" * 60)\n",
    "    print(\"SENSITIVITY AND SPECIFICITY ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    sens_spec = calculate_sensitivity_specificity(\n",
    "        df, true_label_col, pred_label_col, class_names\n",
    "    )\n",
    "    print(f\"{'Class':<15}{'Sens':<10}{'Spec':<10}{'TP':<5}{'TN':<5}{'FP':<5}{'FN':<5}\")\n",
    "    print(\"-\" * 60)\n",
    "    for name, m in sens_spec.items():\n",
    "        print(f\"{name:<15}{m['sensitivity']:<10.4f}{m['specificity']:<10.4f}\"\n",
    "              f\"{m['tp']:<5}{m['tn']:<5}{m['fp']:<5}{m['fn']:<5}\")\n",
    "    # Averages\n",
    "    avg_sens = np.mean([m['sensitivity'] for m in sens_spec.values()])\n",
    "    avg_spec = np.mean([m['specificity'] for m in sens_spec.values()])\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"{'Macro Avg':<15}{avg_sens:<10.4f}{avg_spec:<10.4f}\")\n",
    "    # Weighted\n",
    "    counts = df[true_label_col].value_counts()\n",
    "    total = len(df)\n",
    "    w_sens = sum(m['sensitivity'] * (counts[m['class_value']] / total)\n",
    "                 for m in sens_spec.values())\n",
    "    w_spec = sum(m['specificity'] * (counts[m['class_value']] / total)\n",
    "                 for m in sens_spec.values())\n",
    "    print(f\"{'Weighted':<15}{w_sens:<10.4f}{w_spec:<10.4f}\")\n",
    "\n",
    "    # 4) ROC curve + AUC\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ROC CURVE AND AUC ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    fpr, tpr, roc_auc = {}, {}, {}\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Default colors if None\n",
    "    if colors is None:\n",
    "        colors = ['#ff8d7f', '#84c9ff', '#e4a8ff'] * ((n_classes // 3) + 1)\n",
    "\n",
    "    for i, (cls_val, cls_name, col) in enumerate(zip(classes, class_names, colors)):\n",
    "        # True binary for this class\n",
    "        y_true_i = y_true_bin[:, i]\n",
    "        # Use the real probability for this class\n",
    "        y_score_i = df[pred_score_cols[i]].values\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_true_i, y_score_i)\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=fpr[i], y=tpr[i],\n",
    "            mode='lines',\n",
    "            line=dict(color=col, width=3),\n",
    "            name=f'{cls_name} (AUC={roc_auc[i]:.4f})',\n",
    "            hovertemplate='<b>%{fullData.name}</b><br>FPR: %{x:.4f}<br>TPR: %{y:.4f}<extra></extra>'\n",
    "        ))\n",
    "\n",
    "    # Diagonal\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=[0, 1], y=[0, 1],\n",
    "        mode='lines',\n",
    "        line=dict(color='gray', width=2, dash='dash'),\n",
    "        showlegend=False\n",
    "    ))\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=dict(\n",
    "            text='ROC Curves – Multiclass (One vs Rest)',\n",
    "            font=dict(family=font_family, size=font_size_title),\n",
    "            x=0.5, xanchor='center'\n",
    "        ),\n",
    "        xaxis=dict(\n",
    "            title=dict(text='False Positive Rate (1 − Specificity)',\n",
    "                       font=dict(family=font_family, size=font_size_axes)),\n",
    "            tickfont=dict(family=font_family, size=font_size_ticks),\n",
    "            range=[0, 1], showgrid=True, gridcolor='lightgray',\n",
    "            showline=True, linecolor='black', linewidth=2\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            title=dict(text='True Positive Rate (Sensitivity)',\n",
    "                       font=dict(family=font_family, size=font_size_axes)),\n",
    "            tickfont=dict(family=font_family, size=font_size_ticks),\n",
    "            range=[0, 1], showgrid=True, gridcolor='lightgray',\n",
    "            showline=True, linecolor='black', linewidth=2\n",
    "        ),\n",
    "        showlegend=show_legend,\n",
    "        legend=dict(\n",
    "            font=dict(family=font_family, size=10),\n",
    "            x=0.6, y=0.2,\n",
    "            bgcolor='rgba(255,255,255,0.8)',\n",
    "            bordercolor='black',\n",
    "            borderwidth=1\n",
    "        ),\n",
    "        plot_bgcolor='white',\n",
    "        paper_bgcolor='white',\n",
    "        width=800,\n",
    "        height=600\n",
    "    )\n",
    "\n",
    "    # Compute macro & weighted AUC with sklearn\n",
    "    macro_auc = np.mean(list(roc_auc.values()))\n",
    "    proba_matrix = df[pred_score_cols].values\n",
    "    weighted_auc = roc_auc_score(\n",
    "        y_true_bin,\n",
    "        proba_matrix,\n",
    "        multi_class='ovr',\n",
    "        average='weighted'\n",
    "    )\n",
    "\n",
    "    print(\"\\nAUC Scores per class:\")\n",
    "    for i, name in enumerate(class_names):\n",
    "        cnt = counts[classes[i]]\n",
    "        pct = cnt / total * 100\n",
    "        print(f\"  {name}: {roc_auc[i]:.4f} (n={cnt}, {pct:.1f}%)\")\n",
    "    print(f\"\\nMacro‐Average AUC:    {macro_auc:.4f}\")\n",
    "    print(f\"Weighted‐Average AUC: {weighted_auc:.4f}\")\n",
    "\n",
    "    # Detailed metrics if desired\n",
    "    if print_detailed_metrics:\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"DETAILED METRICS SUMMARY\")\n",
    "        print(\"=\" * 60)\n",
    "        p, r, f1, supp = precision_recall_fscore_support(\n",
    "            df[true_label_col],\n",
    "            df[pred_label_col],\n",
    "            labels=classes,\n",
    "            average=None\n",
    "        )\n",
    "        print(f\"{'Class':<15}{'Prec':<10}{'Rec':<10}{'F1':<10}{'Supp':<8}\")\n",
    "        print(\"-\" * 55)\n",
    "        for i, name in enumerate(class_names):\n",
    "            print(f\"{name:<15}{p[i]:<10.4f}{r[i]:<10.4f}{f1[i]:<10.4f}{supp[i]:<8}\")\n",
    "        p_m, r_m, f1_m, _ = precision_recall_fscore_support(\n",
    "            df[true_label_col],\n",
    "            df[pred_label_col],\n",
    "            average='macro'\n",
    "        )\n",
    "        p_w, r_w, f1_w, _ = precision_recall_fscore_support(\n",
    "            df[true_label_col],\n",
    "            df[pred_label_col],\n",
    "            average='weighted'\n",
    "        )\n",
    "        print(\"-\" * 55)\n",
    "        print(f\"{'Macro Avg':<15}{p_m:<10.4f}{r_m:<10.4f}{f1_m:<10.4f}\")\n",
    "        print(f\"{'Weighted':<15}{p_w:<10.4f}{r_w:<10.4f}{f1_w:<10.4f}\")\n",
    "        print(\"\\n(Note: Recall = Sensitivity)\")\n",
    "\n",
    "    # Save and show\n",
    "    fig.write_image(save_path)\n",
    "    print(f\"\\nROC plot saved as {save_path}\")\n",
    "    fig.show()\n",
    "\n",
    "    return {\n",
    "        'sensitivity_specificity': sens_spec,\n",
    "        'macro_auc': macro_auc,\n",
    "        'weighted_auc': weighted_auc,\n",
    "        'figure': fig\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca54199c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# Build predictions DataFrame and plot ROC + metrics\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# 1) Create a DataFrame with true & predicted labels\n",
    "predictions = df_hold[[target]].copy()\n",
    "predictions['prediction_label'] = y_pred   # your predicted labels from final_clf\n",
    "\n",
    "# 2) Add one Score_* column per class in the order TabNet.classes_\n",
    "for i, cls in enumerate(final_clf.classes_):\n",
    "    predictions[f\"Score_{cls}\"] = y_proba[:, i]  # your predicted probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99eecf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "predictions = pd.read_csv(\".../tabnet_holdout_for_roc.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tabnet_py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
